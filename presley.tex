\title[PRESLEY]{Perceptual Refinement Of An End-to-End Video Streaming Pipeline Via Generative AI Layers}

\newpage \section{Introduction} \label{sec:introduction}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{Figures/New Figures/Overview.pdf}    
%     \caption{Overview of the ELVIS pipeline.}\label{fig:overview}
%     \vspace{-6mm}
% \end{figure}

% With the increasing demand for high-quality video streaming and storage, video compression methods are becoming more crucial than ever. 
% Traditional codecs have significantly reduced file sizes while preserving visual quality, with each new iteration improving upon its predecessor by about 50\%~\cite{li2019codecs}. 
% However, further advancements are needed to meet the growing requirements of bandwidth-constrained environments and the ever-increasing resolution of video content. 
% A promising avenue is represented by neural codecs~\cite{chen2021nerv, chen2023hnerv}, \ie compressing a video into the weights of a neural network, which is then prompted by the client to recreate frames. 
% In addition to server-side compression efficiency, video enhancement techniques have been explored to leverage client-side computation, such as frame interpolation and super-resolution~\cite{niklaus2017video, bao2019depth, wang2018esrgan, ledig2017photo}. 
% % Frame interpolation generates intermediate frames in between existing frames to reduce the number of frames that need to be stored or transmitted, resulting in smoother playback with lower bandwidth usage. 
% % This technique, often used for lower frame rate content, has been shown to improve perceived video quality~\cite{niklaus2017video, bao2019depth}. 
% % However, it faces challenges in accurately predicting motion, especially in dynamic scenes, which can lead to artifacts and distortions that reduce visual fidelity. 
% % Super-resolution focuses on upscaling low-resolution video frames to higher resolutions, improving detail in video content~\cite{wang2018esrgan, ledig2017photo}. 
% % This approach can produce visually impressive results, especially for high-resolution displays, but often requires computationally intensive models, making real-time application challenging~\cite{dong2015image}. 

% All of the aforementioned techniques face a common limitation: they can only tackle low-level features of videos, such as edges and block-wise flow.
% With the advent of large generative models, AI is now able to learn and replicate high-level video features, objects, and up to a few seconds of the whole video~\cite{openai2021dalle}. 
% Therefore, a new and yet to be explored avenue for enhancing video compression is the integration of video in-painting techniques~\cite{propainter, e2fgvi, sora} that analyze the video as a whole, gather context as to what is represented in it, and fill in missing or corrupted regions.
% Using the latest advances in machine learning, such as attention mechanisms~\cite{vaswani2017attention}, and training on increasingly large and curated datasets, state-of-the-art (SOTA) in-painting algorithms learn how objects typically appear and move in videos, giving them the ability to recreate far larger portions of content than previously possible~\cite{propainter, e2fgvi, sora}.

% This paper's contributions are twofold: ($i$) it presents ELVIS, an innovative method that implements video in-painting alongside encoding, aiming to enhance compression efficiency by eliminating parts of the video that are challenging to encode, but can be regenerated by the client using in-painting algorithms, without significantly deteriorating the viewing experience.
% This approach allows the encoder to focus on portions that cannot be easily replicated at the client side, thereby increasing video quality without additional bandwidth requirements.
% The effectiveness of this method is evaluated using a variety of metrics, 
% % including Mean Squared Error (MSE)~\cite{mse}, Peak Signal-to-Noise Ratio (PSNR)~\cite{psnr}, Structural Similarity Index (SSIM)~\cite{ssim}, Video Multimethod Assessment Fusion (VMAF)~\cite{vmaf}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{lpips}, 
% to ensure that the reconstructed video meets the high standards required for practical deployment.
% Contribution ($ii$) is the release~\footnote{Codebase available at~\hyperlink{https://github.com/emanuele-artioli/elvis}{https://github.com/emanuele-artioli/elvis}}
% of an end-to-end pipeline, outlined in Figure~\ref{fig:overview}, that complements video encoders on the server side, with SOTA AI techniques that remove redundancies, enhances the client side decoding with video in-painting techniques, and orchestrates and monitors the performance in terms of resources required and output quality of the whole system. This pipeline is designed to be modular and adaptable, allowing for the easy incorporation of new and improved components, such as new codecs, in-painting models, or video quality metrics. As a result, the proposed architecture not only provides immediate benefits in terms of quality improvements, but also ensures long-term adaptability as better techniques become available.

% The remainder of this paper is structured as follows. Section~\ref{sec:related_work} reviews related work on server- and client side techniques to improve video streaming. Section~\ref{sec:ELVIS} describes ELVIS in detail, including the algorithms and techniques used and the specific procedures employed in the pipeline. Section~\ref{sec:implementation} presents the details of the implementation of ELVIS (incl. design choices), while Section~\ref{sec:evaluation} provides the experimental setup, evaluation metrics, and results of the evaluation. Finally, Section~\ref{sec:conclusions} concludes the paper with a summary of findings and directions for future research.

\newpage \section{Background and Related Work} \label{sec:background}

Video streaming's challenges have traditionally been twofold: ($i$) maximizing encoding efficiency on the server side, 
and ($ii$) quickly adapting to dynamic network conditions on the client side.
Over time, codecs have advanced to the point where further significant reductions in video file size 
often require a tenfold increase in encoding time~\cite{minopoulos2020codecs-for-live}. 
Meanwhile, Adaptive Bitrate (ABR) algorithms have matured to make efficient use of growing network bandwidths. 
With the advent of powerful AI models and mobile processors, it is now possible to share the computational burden between server and client, 
potentially boosting the delivered video quality through client-side enhancements.

\subsection{Redundancy Removal}
Traditional video codecs, such as AVC/H.264~\cite{overviewAVC}, HEVC/H.265~\cite{overviewHEVC}, 
and more recent iterations like VVC/H.266~\cite{overviewVVC} and AV1~\cite{overviewAV1}, 
have achieved significant gains in compression efficiency by minimizing spatial and temporal redundancies, 
\ie searching for similar groups of pixels that can be encoded once and referenced multiple times.
Modern encoders exhaustively search for inter-frame motion matches and intra-frame patterns to minimize repeated information. 
This approach has been highly successful, but it faces diminishing returns. 
Further bitrate reduction increasingly comes at the cost of extreme computational complexity on the server side. 
Moreover, as video resolutions grow (4K, 8K, etc.), real-time encoding with these brute-force techniques becomes challenging. 
Modern video codecs treat video at a low level of abstraction (blocks of pixels), 
which means they do not inherently understand which parts of a scene are semantically important. 
All regions of a frame are largely treated uniformly in the compression process, aside from generic heuristics for smooth vs. detailed areas. 
This uniform treatment can lead to suboptimal allocation of bits. 
For example, wasting bits on background details while important foreground content could benefit from higher fidelity.

\subsection{Content-Aware and Semantic Compression}
Recognizing that not all pixels are equally important, researchers and industry have explored semantic or content-aware streaming techniques 
that prioritize perceptually important regions. 
Many video coding standards provide hooks for region-of-interest (ROI) based encoding. 
For instance, the x264/x265 encoders allow specifying a QP offset map to allocate lower quantization (better quality) to certain regions 
and higher quantization to others. 
In practice, a developer can mark an ROI (e.g., a face or text area) such that the encoder spends more bits there and less on other regions (e.g., the background). 
The FFmpeg framework introduced an addroi filter to facilitate this, attaching ROI metadata (position and quality offset) to video frames. 
However, such features remain half-baked in mainstream tools. 
For example, as of FFmpeg 4.3.1 the ROI filter only supports static regions defined at the command line and cannot be changed per frame, 
limiting its use to trivial cases or demonstrations~\cite{ffmpeg}. 
Numerous research efforts have proposed ROI-based rate control algorithms that automatically detect important regions and adjust encoding on the fly.
These methods (often using object detectors or saliency models) show that boosting ROI quality can improve perceptual results 
with only small bitrate overhead~\cite{roi2014}.
Despite these advances, content-aware encoding has seen little adoption in general video streaming. 
A major reason is the lack of robust, well-documented support in encoders and pipelines.
Integrating semantic analysis into a real-time codec is non-trivial, and existing ROI APIs are sparsely documented and infrequently used. 
Additionally, there are design challenges in generalizing semantic importance: 
what constitutes an “important” region can vary with content and viewer preference, making it hard to automate in a one-size-fits-all manner.

\subsection{Semantic Video Communication}
Beyond ROI-based tweaks to traditional codecs, a more radical class of approaches transmits high-level semantic descriptors instead of pixel data, 
leveraging AI at the client to reconstruct the video. 
In video conferencing, for example, NVIDIA’s Maxine~\cite{nvidia-maxine} system demonstrated that it can send 
only a sparse set of facial keypoints from the sender and use a generative adversarial network (GAN) at the receiver to render a talking-head video, 
reducing bandwidth by up to 10× compared to AVC.
By transmitting who and how someone is moving rather than their raw pixel values, such systems achieve massive compression 
while preserving perceptually important content (in this case, a person’s face and expressions). 
Recent research is extending this idea to more general streaming scenarios.
For example, GenStream~\cite{genstream} transmits a combination of skeletal keypoints for human figures, camera parameters, and a static background model; 
a generative model at the client then reconstructs photorealistic video of the actors and composites them into the scene.
This approach was shown to achieve an extreme compression ratio of over 99\% bandwidth reduction compared to HEVC in a sports video example.
These results underscore the barely tapped potential of semantic understanding: by knowing which aspects of a scene carry the information 
(e.g., human motion vs. static background), one can remove orders of magnitude more redundancy than possible with brute-force pixel matching.
However, semantic streaming techniques are largely confined to prototypes and niche applications at present. 
Video conferencing is a favorable use-case (with constrained scenarios like talking heads or virtual backgrounds), 
but generalizing this to arbitrary video content (movies, user-generated videos, etc.) is an open challenge. 
Robustly analyzing each frame for semantic content in real time, and faithfully reconstructing it on the client, 
requires sophisticated AI integration that current streaming pipelines do not support. 
Moreover, without standardization, there is no interoperability: a generative reconstruction method must be paired with a specific semantic encoder, 
unlike traditional codecs which offer universal decode compatibility. 
In summary, while today’s state-of-the-art codecs squeeze redundancy from videos at the pixel level, 
high-level, \ie semantic, compression remains an emerging frontier. 
Bridging this gap can unlock new gains in perceptual quality at a given bitrate, 
pushing video streaming beyond the constraints of low-level redundancy removal.

\subsection{Video Restoration}
The primary task on the client side is to reconstruct the encoded video in the highest quality.
To achieve this, in addition to employing an efficient decoding algorithm, various techniques can be applied to further enhance the video quality: 
%Two approaches are prominent:

\subsubsection{\textbf{Frame Interpolation}} is a technique to increase a video’s frame rate by generating intermediate frames between existing ones, 
producing smoother and more fluid motion.
This is especially useful for low frame-rate content or creating slow-motion effects, 
where synthesizing additional frames locally can greatly enhance playback without needing higher transmission rates~\cite{niklaus2017video}. 

Modern frame interpolation methods often leverage optical flow to estimate motion between frames. 
For example, Bao~\etal~\cite{bao2019depth} explicitly incorporate depth information to handle occlusions, 
projecting intermediate flows that preferentially sample closer objects to tackle large motions. 
More recently, transformer-based models have been introduced such as Shi~\etal~\cite{shi2022vfit}, 
which use self-attention to capture long-range dependencies and content-aware correlations for improved motion prediction. 

These approaches generally excel at maintaining temporal coherence in the generated frames. 
However, scenes with complex or unpredictable motion remain challenging, 
and even advanced interpolation can produce visible artifacts when motion estimation is imperfect. 
In practice, issues like ghosting, motion blur, or other distortion artifacts may appear if the algorithms misinterpret object motion. 
Such artifacts ultimately compromise visual fidelity in highly dynamic sequences.

\subsubsection{\textbf{Super-Resolution}} aims to reconstruct high-resolution video frames from low-resolution inputs, enriching details and sharpness while preserving temporal consistency across frames. 

Traditional approaches range from upscaling filters (\eg bicubic, Lanczos) to modern deep learning methods~\cite{opencv-bicubic, opencv-lanczos}. 
Enhanced Super-Resolution GAN (ESRGAN) exemplifies the leap in quality achieved with adversarial learning~\cite{wang2018esrgan}. 
It produces consistently higher-quality frames with more realistic and natural textures than earlier CNN-based methods by focusing on perceptual detail and texture refinement. 

Recent video super-resolution models build on both convolutional and transformer architectures to improve detail restoration and temporal coherence. 
For instance, the Video Super-Resolution Transformer introduced by Cao~\etal~\cite{cao2021video} processes spatial and temporal information with self-attention, and newer multi-scale attention mechanisms can effectively capture fine details at various scales.
These innovations have led to sharper textures and fewer artifacts in output videos compared to applying single-image super-resolution on each frame independently.
Despite these advances, the computational cost of state-of-the-art super-resolution (and frame interpolation) models is a notable barrier. 
Many require heavy neural networks or complex temporal modeling (optical flow, 3D convolutions, \etc), which makes real-time deployment difficult for live streaming scenarios. 
Research has begun exploring efficiency improvements, but in practical terms the high computational overhead still limits widespread use in latency-sensitive applications.

Frame interpolation and super-resolution have also been used together in joint frameworks. 
By simultaneously increasing temporal and spatial resolution, these hybrid methods produce more coherent enhancements. 
For example, upscaling a 2K 30fps video to 4K at 60fps by applying super-resolution and interpolation in tandem can yield significant improvements in visual quality. 
Such a joint approach can improve overall perceptual quality and motion smoothness beyond using super-resolution alone~\cite{kim2020fisr}.
However, these techniques still largely operate on low-level visual features (\ie pixel textures, edges, and local motion) and lack understanding of the high-level content in a scene. 
Consequently, their effectiveness plateaus in more complex scenarios. 
They struggle to reconstruct occluded or unseen content and cannot reason about object semantics, often resulting in artifacts despite the enhanced resolution and frame rate. 
For example, even after applying interpolation and upscaling, fast-moving or heavily occluded regions may exhibit ghosting artifacts or unnatural texture movements

\subsubsection{\textbf{Video Inpainting}} addresses the restoration challenge from a different angle: rather than only refining what is already encoded, it aims to reconstruct missing or occluded regions in video frames by learning from the surrounding spatio-temporal context. 

This technique has a long history, with early approaches dating back decades. 
Initial methods extended image inpainting via texture synthesis and patch-based copying to videos.
Notable examples include exemplar-based texture synthesis for filling holes in images and simple extensions to video.
Patwardhan et al. (2005), for instance, proposed filling in occluded objects by propagating patches from neighboring frames. 
However, such methods struggled to maintain realism over time: spatial patches could be filled reasonably in individual frames, but ensuring temporal consistency across consecutive frames was difficult. 
As a result, early video inpainting often produced flickering artifacts and implausible reconstructions when objects or backgrounds were in motion.
Some approaches introduced global optimization to enforce consistency (e.g. Wexler et al. 2007 used a global space-time energy minimization over patches), but these were computationally expensive and still not robust for complex dynamic scenes.

Recent advancements in generative AI have revolutionized video inpainting, enabling far more coherent and contextually plausible restorations. 
Modern video inpainting models are both large enough and efficient enough to capture complex spatial and temporal dependencies within their receptive field.
For example, Spatial-Temporal Transformer Networks (STTN)~\cite{zeng2020sttn} use self-attention mechanisms to fill missing regions jointly across all frames, rather than frame-by-frame. 
The model’s spatial attention focuses on textures and structures within each frame while temporal attention draws correspondences between frames, so that the filled content not only looks realistic in a single frame but also remains consistent over time.
By optimizing with a combined spatial-temporal adversarial loss, STTN can ensure that inpainted regions integrate smoothly with both the current frame’s details and the video’s motion across frames
Flow-guided video inpainting has also shown remarkable success. 

Methods like Flow-Guided Video Completion (FGVC)~\cite{gu2023flow} and the end-to-end flow-guided model E2FGVI~\cite{e2fgvi} explicitly estimate optical flow in missing regions to guide the propagation of content through the video. 
By completing and leveraging optical flow, these approaches can accurately propagate moving objects and background across holes, preserving motion continuity even in high-action scenes with complex movement. 
For instance, a recently introduced flow-guided diffusion model for video inpainting achieved significantly improved temporal consistency, ensuring that the generated content does not jitter or tear when objects move
Essentially, the optical flow maps act as a blueprint for how pixels should move from frame to frame, so the inpainting network can fill in content that moves in unison with the surrounding video

Generative models based on adversarial training have further bolstered video inpainting. 
ProPainter~\cite{propainter} is an example that combines a propagation module with a transformer-based generator. 
By warping known regions in both image and feature space (dual-domain propagation) and then using a mask-guided video transformer, ProPainter is able to synthesize highly realistic content in place of missing regions.
Crucially, methods like this utilize GAN principles: a generator network fills in the holes while a discriminator network evaluates the realism of the result, pushing the generator to produce plausible textures and structures that match the scene’s context. 
This adversarial process helps ensure that inpainted areas blend seamlessly with their surroundings, respecting object boundaries and lighting so that the viewer may not even detect that anything was missing. 

Diffusion models have also emerged as powerful tools for video inpainting. 
Instead of generating a frame in one pass, diffusion-based approaches (such as in E2FGVI) start with a noisy frame and iteratively refine it, guided by the context from unmasked regions and often by optical flow or other priors. 
This iterative denoising allows for high-fidelity completions, as the model can gradually introduce fine details that conform to both spatial textures and temporal dynamics. 
By integrating temporal constraints into the diffusion process, these models effectively suppress flicker and ensure that each frame’s output remains consistent with the next, even for very complex scenes.
To train these advanced inpainting models, researchers employ sophisticated loss functions and strategies that specifically enforce realism and consistency. 

In addition to reconstruction losses that ensure pixel-level accuracy, temporal consistency losses are used to penalize discrepancies across frames. 
For example, some models use optical-flow based losses: they ensure that if an object is inpainted in one frame, when that frame is warped to the next using the video’s flow, it should match the inpainted content in the next frame. 
Others incorporate adversarial losses not just spatially (on single frames) but temporally.
A discriminator might look at sequences of frames and try to distinguish real vs. generated video clips, thereby forcing the generator to produce sequences without glaring temporal seams. 
Techniques like recurrent feedback or memory modules can also directly enforce that the network’s output at one time step informs the next, thereby greatly reducing flicker.
Thanks to these innovations, state-of-the-art video inpainting can restore missing content with both high detail and high temporal coherence.
In practice, this means even fast-moving objects or complex backgrounds can be filled in convincingly: a previously occluded object can appear from behind another with correct form and motion, or a hole in a textured background (like a patterned wall or flowing water) can be patched in a way that the pattern continues unbroken over time.


\subsection{Video Quality Assessment}
Video quality is traditionally evaluated using full-reference metrics such as Mean Squared Error (MSE)~\cite{mse}, Peak Signal-to-Noise Ratio (PSNR)~\cite{psnr}, Structural Similarity Index (SSIM)~\cite{ssim}, and Video Multi-method Assessment Fusion (VMAF)~\cite{vmaf}.
These metrics assess reconstruction fidelity based on pixel-wise accuracy, structural alignment, and human-validated perceptual quality. 
VMAF, in particular, has become a widely adopted industry benchmark, combining multiple quality indicators via machine learning and validated extensively on large-scale datasets~\cite{pastor2023av1, ullah2023image}.
However, despite their broad usage, these traditional metrics often struggle to capture the perceptual fidelity of generative or inpainted content, especially when artifact types differ significantly from those encountered in conventional codec-based degradation~\cite{onuoha2023field, hammou2023comparison}.

\subsubsection{Full-Reference metrics}
In AI-enhanced pipelines, especially those involving inpainting, super-resolution, or semantic restoration, the distortions introduced are perceptually plausible but structurally misaligned. 
For instance, an inpainted object might differ in shape or texture from the ground truth but still appear realistic to human observers. 
Traditional metrics, being highly sensitive to pixel-level differences, may incorrectly penalize such outputs. 
This misalignment motivates the use of learned perceptual metrics, which compare higher-level semantic features extracted by deep neural networks.
One such metric is the Learned Perceptual Image Patch Similarity (LPIPS)~\cite{lpips}, which computes similarity in feature space rather than pixel space. 
LPIPS compares deep embeddings (e.g., from VGG or AlexNet) between patches of reference and generated frames, capturing perceptual cues such as texture continuity, object coherence, and visual realism. 
Studies have shown that LPIPS correlates better with human opinion scores on GAN-generated or inpainted content~\cite{hou2022perceptual}, making it a crucial tool for assessing outputs from generative pipelines where traditional metrics fall short.
An important ongoing challenge is standardization and interpretability. 
While LPIPS and similar learned metrics show strong perceptual correlation, the lack of frame-level temporal consistency checks means they may overlook flickering or dynamic artifacts common in generative video. 

\subsubsection{No-Reference metrics}
A notable limitation of most traditional and perceptual quality metrics such as PSNR, SSIM, VMAF, and even LPIPS, is their reliance on a complete and semantically aligned reference. 
This creates challenges when evaluating partially restored or generatively enhanced videos, where localized regions may differ structurally from the original despite being perceptually valid. 
In such cases, full-reference metrics often penalize acceptable or even high-quality results due to strict pixel- or feature-level discrepancies.
Early no-reference approaches such as BRISQUE~\cite{mittal2012no} evaluate spatial quality by modeling deviations from natural scene statistics, offering fast, lightweight assessment, but are limited in their sensitivity to artifacts introduced by neural generation or temporal inconsistency.
To better address the perceptual characteristics of generated content, metrics such as the Fréchet Inception Distance (FID) have become widely used, particularly in the image and video synthesis community. 
FID~\cite{heusel2017gans} measures the distributional distance between real and generated feature embeddings, offering a statistical proxy for realism. 
While effective for comparing entire datasets of generated content, FID is less suited to localized or frame-level quality analysis and does not directly capture temporal coherence.
Building on this, the Feature-Vector Motion Distance (FVMD)~\cite{fvmd} was developed to evaluate temporal consistency in generated or perceptually enhanced videos. 
FVMD computes the distance between high-level feature vectors extracted from consecutive frames using pretrained video encoders (e.g., I3D), allowing it to detect motion artifacts such as flickering, jitter, or inconsistent dynamics that may not be evident in static image metrics.

In conclusion, while full-reference metrics remain valuable for measuring fidelity to the original signal, they often fail to reflect perceptual quality in the presence of generative artifacts, penalizing outputs that differ from the ground truth but are visually acceptable. 
Conversely, no-reference metrics offer flexibility by evaluating realism or temporal consistency without requiring a reference, but they disregard adherence to the original content, making them unsuitable for judging semantic fidelity or preservation of intended details. 
In the absence of a standardized hybrid metric that jointly considers both perceptual plausibility and ground-truth alignment, combining full-reference and no-reference approaches remains a practical and effective strategy to comprehensively assess the quality of enhanced video.

\newpage \section{System Design} \label{sec:design}

This work is, at its core, an optimization layer on top of a traditional video streaming pipeline, 
\ie, a search for redundant data that can be removed from a video before encoding to obtain a significant bitrate reduction, 
and restored after decoding with a quality loss that is deemed acceptable compared to the reduction in size.
Given that videos are encoded based on blocks, \ie squares of pixels of variable size, 
the task can be translated into a search for redundant blocks, 
which are then strategically degraded to optimize for bitrate while enabling successful restoration. 

\subsection{Removability Score Computation} 

To start our investigation into which blocks are best to degrade, we must consider the trade-offs involved. 
Degrading complex blocks will lower the bitrate required to encode a video at a certain resolution more significantly than degrading simple blocks, 
but will also make recreating such blocks harder for the restoration tool. 
Indeed, this trade-off is a central problem in any kind of media encoder.
It is also a crucial reason why generative models have the potential to revolutionize video coding. 
State-of-the-art video codecs must include all the information about a video inside the encoded file, even though such information is often redundant. 
As an example of this, consider a human being: we almost all have the same number of limbs, moving in the same way and performing repetitive actions. 
The same is true for almost every object commonly seen in videos. 
Large generative models contain information about such shapes and actions and can, therefore, reconstruct them from partial information. 
Therefore, ELVIS solves the complexity trade-off by degrading complex blocks from common objects 
and letting the restoration model reconstruct them based on spatial and temporal neighboring blocks.

However, generative models are still in their infancy, and their contexts and reconstructive capabilities are limited. 
Thus, we improve our performance by prioritizing degrading blocks from the background and limiting degradation from the main object in the picture, 
where the viewer's eyes focus and are therefore more sensitive to even small errors and inconsistencies. 

Instructed by these two ideas, we compute removability scores to assess which parts of the video are more expendable. 
This requires: 

\begin{itemize} 
    \item[i] a method to extract temporal complexity (TC) and spatial complexity (SC) tensors for each block of each frame, and 
    \item[ii] a segmentation tool to determine which regions belong to the visual foreground (e.g., objects of interest) versus background. 
\end{itemize}

\begin{figure}[b]
    \centering  
    \includegraphics[width=\columnwidth]{Figures/New Figures/shrinking.pdf}
    \caption{Complexity calculation and frame shrinking.}
    \label{fig:shrinking}
\end{figure}

The spatial complexity tensor measures the level of detail and variability within each block in a frame, 
with higher values indicating intricate textures or fine details. 
Temporal complexity, on the other hand, evaluates the degree of motion or change between consecutive blocks, 
with higher values denoting rapid or irregular movements.
These spatial complexity and temporal complexity tensors have shape: $(I, J, N)$, 
where $I$ is the number of blocks per row in a frame, $J$ the number of blocks per column, and $N$ the number of frames.
The 3D tensors can be sliced along the frame dimension into two 2D matrices $S_{i,j}$ and $T_{i,j}$, as shown in Figure~\ref{fig:shrinking}, step (1-2).

To evaluate ($ii$), we leverage masks that signal which blocks belong to the foreground, as depicted in Figure~\ref{fig:shrinking}, step (3).
The removal of these foreground blocks needs to be deprioritized compared to background blocks, 
since introducing artifacts in the foreground has a disproportionate impact on viewer satisfaction.

\subsection{Degradation Strategy} 

In the ELVIS pipeline, blocks with the highest removability scores are selected for removal, based on a predefined removal ratio $r$.
Current codecs require video frames of constant, rectangular shape; therefore, blocks need to be removed in such a way that respects this. 
Two approaches are possible: one is to remove blocks independently of each other 
and then rearrange them so that the result forms rectangular frames, all of the same resolution. 
For example, a frame where fewer blocks were removed, could "lend" blocks to a frame that was impacted more heavily by block removal. 
However, moving blocks from one frame to another, or from one position to another in the same frame, breaks the video continuity, 
and thus reduces the efficiency of codecs, which then need to overcome this discontinuity with higher information count, defeating the purpose.
Another approach is to replace removed blocks with placeholders, \eg black blocks.
Said approach would work if the black blocks could be encoded at virtually no information overhead, 
but this is not the way modern codecs are implemented.
Indeed, there is little gain in encoding efficiency in replacing an image block with a black one, 
unless that black block is repeated multiple times in rapid sequence so that it can be referenced. 
ELVIS solves this problem by iteratively shrinking frames one row at a time, removing the block at highest importance, 
until the frame is shrunk by the amount defined in the configuration parameters.
Thus, at each iteration, each frame is one block narrower, and can be shrunk in a way that maintains both the shape 
and the visual consistency required by codecs, as shown in Figure~\ref{fig:shrinking}, step (4).

The major conceptual contribution of PRESLEY over ELVIS is a more nuanced, adaptive degradation scheme that preserves the full frame shape. 
Instead of removing blocks, PRESLEY reduces the visual complexity of each block in proportion to its removability score. 
The system does not require any frame shrinking, simplifying integration with existing codecs and streaming infrastructures.
Furthermore, by retaining all blocks, albeit in a degraded form, the system preserves the overall scene structure, 
which can be beneficial for restoration tasks.
On the other hand, this approach requires a tighter integration between the degradation strategy and the restoration tool, 
as the latter must be capable of leveraging the retained structural information, and not be confused by it.
To achieve this, PRESLEY supports multiple degradation modalities: 

\begin{itemize} 
    \item[QP Mapping] 
    For codecs that support quantization parameter (QP) maps, PRESLEY generates block-wise QP offsets derived from the removability scores.
    \item[Downsampling] 
    Blocks with high removability scores may be downsampled by varying factors before encoding, 
    resulting in spatial simplification and reduced bitrate demand.
    \item[Noise Injection] 
    As an alternative degradation channel, Gaussian noise of variable strength, again proportional to removability, 
    is introduced into less important blocks, simulating perceptual loss that can later be targeted by generative restoration.
\end{itemize} 

These strategies are codec-agnostic in spirit: while QP maps require specific codec support, 
downsampling and noise-based degradation can be applied independently or combined, depending on the desired bitrate-quality trade-off 
and reconstruction capabilities.

Together, ELVIS and PRESLEY offer two complementary paradigms for video simplification. 
ELVIS achieves compression through block removal, while PRESLEY achieves it through adaptive degradation. 
Both rely on the same removability computation, enabling shared infrastructure and evaluation 
while exploring distinct design approaches in compression-aware streaming.

\subsection{Restoration Strategy}

\begin{figure}[t] 
    \centering
    \includegraphics[width=\columnwidth]{Figures/New Figures/stretching&inpainting.pdf}
    \caption{Frame stretching and in-painting.}
    \label{fig:stretching}
\end{figure}

The second part of the pipeline involves performing the inverse process of degradation by restoring the video back to full visual quality. 
Restoration is driven by the degradation approach employed before encoding and relies on the metadata generated by it.

In the ELVIS design, frames are physically shrunk during encoding by removing entire blocks.
Restoration thus requires decoding the received shrunk file, stretching the frames back to full resolution based on the metadata 
(adding placeholder blocks in place of the removed ones), and passing the video to an in-painting model to generate the missing content, 
as illustrated in Figure~\ref{fig:stretching}.

Unlike ELVIS, PRESLEY preserves all blocks but adaptively degrades them, 
meaning each block retains its position and structure but may suffer different degrees of quality loss. 
Consequently, restoration is performed via adaptive enhancement tools, tailored to the specific type of degradation introduced.

\subsubsection{Super-Resolution} 

When blocks have been downsampled at the server, PRESLEY restores them through progressive super-resolution guided by the degradation metadata. 
The process unfolds in rounds, from lowest to highest resolution:

\begin{itemize}
    \item First, the frame is globally downscaled to match the maximum downsampling level.
    \item A super-resolution model is applied to upsample the full frame by 2x.
    \item At each round, blocks corresponding to that scale or higher are extracted and retained.
    \item The process is repeated until all blocks are restored to their intended resolution.
\end{itemize}

\subsubsection{Deblurring}

For blocks degraded via Gaussian noise injection, PRESLEY performs restoration using iterative generative deblurring. 
As with super-resolution, the process is conducted in adaptive rounds:

\begin{itemize}
    \item The initial frame is processed globally using a trained deblurring model.
    \item After each round, blocks whose degradation severity falls within the current level are finalized.
    \item Subsequent rounds target progressively less degraded blocks, refining their clarity based on the preserved image context.
\end{itemize}

With this adaptive approach, blocks benefit from contextual information in neighboring regions during each round of restoration, 
while avoiding over-processing of blocks that have reached their target quality.

\newpage \section{Implementation} \label{sec:implementation}

Implementing the outlined design strategies requires a scalable and modular system, 
to allow future components to be included with minimal reworking of ELVIS.
We, therefore, organize the project as a hierarchy of Python and Bash scripts, 
which perform experiments by comparing videos degraded and restored with different approaches, 
with each other and with a baseline video not processed by AI. 
One of the first challenges encountered when implementing such a system, is the creation of a unified runtime environment, 
containing all the requirements for each component that needs to be called during the different experiments. 
Given the large number of components implemented, requirements from two or more of them conflicted often.
In ELVIS, we partially overcome this limitation by creating a dedicated environment for each combination of components 
so that each is only concerned with a small number of components and can run seamlessly.
In the development of PRESLEY, we solved the issue completely by forking several third-party repositories 
and tweaking them directly.
A bonus of this admittedly time-consuming approach is that we could then easily turn said modules in PyPI-compliant packages,
simplifying the reproducibility and extendability of our approach greatly.

While the degradation strategies of ELVIS and PRESLEY diverge significantly, the overall framework is shared 
and consists of server-side preprocessing, selective degradation, network transmission, and client-side restoration, 
with performance monitoring and an orchestrator overseeing all components. 
This section details the implementation of both server- and client-side pipelines, 
including the tools, libraries, and algorithms used for each stage.

\subsection{Configuration}

\begin{table}[t]
\caption{ELVIS/PRESLEY configuration parameters and possible instantiations.}

\begin{tabular}{lll}
\hline
Parameter        & Values (ELVIS) & Values (PRESLEY)          \\ \hline
Video            & DAVIS dataset  & DAVIS dataset   \\
Block Size       & 8, 16, 32, 64  & 8, 16, 32, 64       \\
Removed Fraction & 0.25, 0.50, 0.75 & 0.25, 0.50, 0.75     \\
$\alpha$         & 0, 0.25, 0.50, 0.75, 1 & 0, 0.25, 0.50, 0.75, 1 \\
$\beta$          & 0, 0.25, 0.50, 0.75, 1 & 0, 0.25, 0.50, 0.75, 1 \\
Height           & 360, 540, 720, 900 & 360, 720, 1080, 2160     \\
Width            & 640, 960, 1280, 1600 & 640, 1280, 1920, 3840   \\
Codec            & H.264, HNeRV & H.265 \\
In-painter Tool  & ProPainter, E2FGVI & ProPainter, E2FGVI \\
Restoration Model & --- & Real-ESRGAN, InstantIR
\end{tabular}
\label{tab:parameters}
\end{table}

The entry point of the project is the \textit{run.sh} script, 
where experiments are set up via numerous parameters, listed in Table~\ref{tab:parameters}.
On top of those general, project-level parameters, each module has internal parameters that can be set, 
mostly to trade-off quality and execution speed, but optimization efforts of those is deemed out of scope.
These parameters can be set not only as scalar values but also as lists of values.
For each list, an item is selected at random as a value for that parameter at each experiment iteration.

Before running an experiment with a particular parameter configuration, consistency rules are evaluated, 
such as whether the product of block size and the fraction to be removed, 
\ie the number of pixels that will be removed from the width of the video, is actually smaller than the width itself. 
Otherwise, there would remain no video to process.
If any rule fails, \textit{run.sh} prints the reason and stops so it can be run again with a different set of arguments.
If every rule is passed, based on the selected codec and models, 
the relevant list of all selected arguments for the current experiment is passed to the next script in the hierarchy, 
\textit{orchestrator.py}, which contains the step-by-step logic of ELVIS and PRESLEY. 
Naturally, if a specific experiment needs to be run, 
it is sufficient to put its desired values as the only possible configuration in parameter lists.

In this implementation, the server and client sides are implemented one after the other in \textit{orchestrator.py} for simplicity.
A real deployment would require different server and client machines, and a real network between them. 
However, to ensure that this simplification does not affect results in any way, 
the client part of the code does not rely on any information (variables, folders, etc.) 
aside from what would be sent over the network in a real application, \ie, compressed video and metadata file.
\textit{orchestrator.py} receives a variable number of arguments from the run script 
and parses them into the correct variables based on which codec and models were chosen. 
Finally, it starts the server side of ELVIS. 

\subsection{Server-Side Degradation Pipeline}

The server extracts raw frames from the chosen video, and scales them to the required resolution by using FFmpeg~\cite{ffmpeg}, 
saving it in a benchmark folder. 
Note that these frames will be used for both benchmark video and as a starting point to generate shrunk frames.
FFmpeg is also used to encode the benchmark frames losslessly into a reference video. 
The reference video will be used to evaluate the distortion introduced by the degradations, 
for both baseline encoding and videos that will go through generative restoration.

\subsubsection{Removability Scores Computation}

\begin{algorithm}[t] 
\caption{Calculate removability score}
\label{alg:calculate_removability}
\begin{algorithmic}[1]
\Require $T, S \in \mathbb{R}^{I \times J \times N}$: Temporal and Spatial complexity tensor
% \Require $S \in \mathbb{R}^{I \times J \times N}$: Spatial complexity tensor
\Require $w,h$: Dimensions of the video
\Require $b$: Block size
\Require $\alpha \in [0,1]$: Weighting factor for spatial and temporal importance
\Require $\beta \in [0,1]$: Smoothing factor
\Require $r \in [0,1]$: Fraction of blocks to remove
\Require $M \in \mathbb{R}^{I \times J \times N}$: segmentation masks
\State Instantiate $C \in \mathbb{R}^{I \times J \times N}$ \Comment{Importance tensor}
\For{$n = 0$ \textbf{to} $N - 1$}
    \If{$n = N - 1$}
        \State $C_{:,:,n} \gets S_{:,:,n}$
    \Else
        \State $C_{:,:,n} \gets \alpha \cdot S_{:,:,n} + (1 - \alpha) \cdot T_{:,:,n}$ \Comment{Equation~\ref{eq:i1}}
    \EndIf
\EndFor
\For{$n = 0$ \textbf{to} $N - 1$}
    \For{$i = 0$ \textbf{to} $I$}
        \For{$j = 0$ \textbf{to} $J$}
            \If{$M_{i,j,n} > 0$}
                $C_{i,j,n} *= -1$ \Comment{Invert importance}
            \EndIf
        \EndFor
    \EndFor
    \For{$i = 0$ \textbf{to} $I - 1$}
        \State Instantiate $row \in \mathbb{R}^{J}$
        \State $row \gets C_{i,:,n}$
        \If{$C_{i,:,n-1} \neq None$}
            \State $row \gets \beta \cdot row + (1 - \beta) \cdot C_{i,:,n-1}$ \Comment{Equation~\ref{eq:i2}}
        \EndIf 
    \EndFor 
\EndFor
\State \Return $C$
\end{algorithmic}
\end{algorithm}

Given the block size chosen for the experiment, the next step is to calculate the aforementioned block-wise spatial complexity (SC)
and temporal complexity (TC) matrices, for which task we chose the Enhanced Video Complexity Analyzer (EVCA)~\cite{evca}.
The complexity of each block is determined by aggregating its spatial and temporal complexity 
using a weighted formula shown in Eq.~\ref{eq:i1}:

\begin{equation}
    C_{i,j} = \alpha \cdot S_{i,j} + (1 - \alpha) \cdot T_{i,j}
    \label{eq:i1}
\end{equation}

where $\alpha$ is a configuration parameter controlling the relative influence of spatial and temporal factors. 
This flexibility allows ELVIS to determine the optimal influence of spatial vs. temporal complexity on a per experiment case.
Temporal complexity for a frame depends on information from the subsequent frame, 
making the last frame solely reliant on spatial complexity.

To further refine block degradation decisions, the final removability metric is smoothed across frames 
using the formula shown in Eq.~\ref{eq:i2}:

\begin{equation}
    C_{i, j} = \beta \cdot C_{i, j} + (1 - \beta) \cdot C_{i-1, j}
    \label{eq:i2}
\end{equation}

where $\beta$ is another tunable parameter that controls the degree of temporal smoothing. 
Low values promote similar degradation decisions on subsequent frames, avoiding flickering on fast moving scenes, 
while for $\beta$ close to \num{1}, past decisions are disregarded, ensuring that on slow-moving scenes, 
restoration models see every block every few frames.

Additionally, foreground-background segmentation is performed using object detection models, 
which take the frames of a video, and return a mask for each frame, $M_{i, j}$, 
delineating the central object (in Figure~\ref{fig:shrinking}, this is represented by the Bunny) from the background 
by identifying blocks where the object is present with 1, and blocks where it is absent with 0.
The complexity score of these blocks is multiplied by -1.
This serves two purposes: one is to render all the background blocks more important than the object blocks, 
therefore giving them priority in the degradation.
This first purpose is applicable to both ELVIS block removal and PRESLEY adaptive degradation.
The second purpose is specifically targeted at ELVIS, and is to make sure that, 
in case the block removal algorithm tried to remove more blocks than the background spans, 
it would start degrading object blocks from the least complex, instead of from the most complex, 
as is the case for background blocks. 
For example, in the picture shown in Figure~\ref{fig:shrinking}, the bottom row has been split in four blocks, 
two of which contain part of the Bunny (foreground), and therefore the remaining two are background.
In case the removed fraction was, for example, 75\%, Algorithm~\ref{alg:calculate_removability} 
would require three blocks to be removed from that row, and having prioritize the highest complexity background block, 
then the lower complexity background block, it would still need to choose another block, 
necessarily among the foreground blocks. 
In this case, the foreground block with lowest complexity would be chosen. 
This is crucial, as background blocks are less important to viewers, since they are typically not being looked at, 
and thus targeting complex background blocks is a good trade-off between bitrate reduction and quality degradation. 
In contrast, removing blocks from the object in focus will likely reduce the perceptual video quality significantly. 
If blocks need to be removed from the object, it is, therefore, preferable to remove low-complexity blocks, 
which the in-painter model can more easily replicate, keeping subjective quality degradation at a minimum.

In the current implementation, we use UFO~\cite{ufo} for foreground detection, but other approaches, 
such as DINO~\cite{dino}- or RF-DETR~\cite{rf-detr}-based segmenters can be easily integrated.

\subsubsection{ELVIS: Block Removal}

\begin{algorithm}[t]
\caption{Frame Shrinking}
\label{alg:shrink_frames}
\begin{algorithmic}[1]
\Require $c$: Color channels (3)
\Require $F \in \mathbb{R}^{I \times J \times c}$: Input frame
\Require $n$: Frame number
\Require $b$: Block size
\Require $R \in \mathbb{R}^{I \times J \times N}$: Coordinate tensor of blocks to remove
\Ensure $F' \in \mathbb{R}^{I \times J-r \cdot J}$: Shrunk frame
\Ensure $O \in \mathbb{R}^{I/r \times J/r \times N}$: Binary mask tensor

\State Instantiate $L \in \mathbb{R}^{I/b \times J/b \times b \times b \times c}$
\For{$i = 0$ \textbf{to} $I/b$}
    \For{$j = 0$ \textbf{to} $J/b$}
        \State $L_{i, j} \gets F_{i \cdot b:(i+1) \cdot b, j \cdot b:(j+1) \cdot b, :}$
    \EndFor
\EndFor

\For{$i = 0$ \textbf{to} $I/b$}
    \For{$j = 0$ \textbf{to} $J/b$}
        \If{$j \notin R_{i, j, n}$}
            \State $F'_{i, j} \gets F_{i, j}$ \Comment{Fill shrunk frame with frame block}
            \State $O_{i, j, n} \gets 0$ \Comment{Signal in mask that block was filled}
        \Else
            \State $O_{i, j, n} \gets 1$ \Comment{Signal that block was not filled}
        \EndIf
    \EndFor
\EndFor
\State \Return $F'_{i, j}$
\State \Return $O_{i, j, n}$
\end{algorithmic}
\end{algorithm}

All the information needed to shrink frames appropriately is contained in the removability scores tensor, 
therefore the process of actually removing them can be highly sped up via parallelization.
In fact, we leverage the ProcessPoolExecutor from the concurrent~\cite{concurrent} Python package, to send each frame to a different thread. 
Each thread executes Algorithm~\ref{alg:shrink_frames}.

This algorithm takes as input frame $n$, loops through each block $j$ of each row $i$, and if its index is not present in the $R$ tensor of blocks to be removed, adds it to the corresponding position of the shrunk frame $F'_{i, j}$.
At the same time, it appends $0$ in the corresponding position of the masks tensor $O_{i, j, n}$, to signal that the block was filled.
Otherwise, if the block index is present in $R$, Algorithm~\ref{alg:shrink_frames} removes the corresponding block (does not add it to the shrunk frame) and signals this with a 1 in $O_{i, j, n}$.
Finally, it outputs its shrunk frame and the metadata tensor.

\subsubsection{ELVIS: Encoding}

When it comes to encoding, \textit{orchestrator.py} uses FFmpeg's FFprobe~\cite{ffmpeg} 
to get the width and height of the shrunk video, then calculates the bitrate of the video. 
The bitrate is calculated by the Python script \textit{calculate\_bitrate.py}, 
which uses a bitrate ladder's combination of resolution and bitrate as data points 
to generate a polynomial curve that best approximates the continuous space of valid possibilities. 
We must generate a continuous curve as the resolution of the shrunk video is not limited to those of traditional videos, 
but can take any value, depending on the combination of block size, initial video resolution, and amount of block removal.
It is important, for such a curve to have a good fit with video encoding's best practices, 
that the input ladder has as many relevant data points as possible; therefore, we leverage the large bitrate ladder 
coming from ARTEMIS's mega-manifest~\cite{artemis}. 
Having obtained the bitrate that best supports the shrunk video resolution, 
without limiting its compression to lower quality but also without wasting bandwidth not translated into higher quality, 
the video can be encoded with an encoder of choice. 
Here lies one of the strengths of ELVIS: its agnosticity to the chosen encoder.
The project is developed in such a way that encoders and in-painters are isolated, and can be swapped easily. 
In the present case, we have included both the most common traditional encoder, namely AVC, 
and the newer iteration of learned codecs, HNeRV. 
We consider these encoders to be a sufficient sample to showcase the potential and versatility of ELVIS, 
but we will be looking into testing new components in the future. 

In case AVC is chosen, the shrunk frames are simply encoded via FFmpeg at the calculated bitrate and saved as an MP4 video. 
The same bitrate will then be used also for the baseline video, to ensure a fair comparison. 
Note that the benchmark video has a higher resolution, since no blocks were removed from it. 
This will translate into a less-than-optimal encoding quality, as the resolution will be encoded at a bitrate 
that is not optimal for it.
Instead of a drawback, we consider this an opportunity, as it allows us to evaluate whether in-painting 
can do a better job at improving video quality, in the presence of bottlenecked bandwidth.

In case HNeRV is chosen, we cannot use the bitrate directly to encode the video, as the encoding is not a video, 
but a neural network representing the video. 
Instead, the orchestrator uses FFprobe again to find the reference video duration, then takes the video duration, 
together with the obtained bitrate, to calculate the size that the video file would have, 
if it were encoded with traditional codecs. 
This is the target size of the encoded neural network that HNeRV will produce, 
and therefore ensures a fair comparison even between traditional and learned codecs.
Another challenge of HNeRV is that it cannot encode every video resolution, but only specific combinations of height and width. 
In our experiments, we were only able to produce encodings of videos 
with a resolution multiple of \numproduct{320 x 320} pixel blocks. 
This severely limits the search space, especially when considering that in-painters's memory requirements scale with resolution, 
and make it impossible to in-paint on videos that are very large. 
Unfortunately, we could not reach the authors for details on whether this limitation can be overcome inside of HNeRV. 
Therefore, we overcame it by padding the benchmark 
and shrinking frames with black bands on the right and bottom sides of the video to reach the necessary resolution. 
The minimum amount of padding required is calculated via Equations~\ref{eq:i3} and~\ref{eq:i4}.

\begin{equation}
    padRight = 320 \cdot \lceil \frac{width}{320} \rceil - width
    \label{eq:i3}
\end{equation}

\begin{equation}
    padBottom = 320 \cdot \lceil \frac{height}{320} \rceil - height
    \label{eq:i4}
\end{equation}

The padding is done frame by frame by FFmpeg, and the result is saved in the benchmark-padded 
and shrunk-padded folders respectively. 
This does add complexity to the video, but initial experiments showed how its magnitude seems to be negligible for HNeRV. 
A deeper study into the issue is considered out of the scope of the present work.
Finally, the padded frames can be encoded with HNeRV, and the neural network encoding representing the shrunk frames 
is sent to the client, together with the in-painting masks generated at the shrinking stage.

\subsubsection{PRESLEY: Adaptive Degradation}

Rather than removing blocks, PRESLEY degrades blocks to varying extents based on their removability scores:

\begin{equation}
    QP_{i,j} = QP_{base} + (QP_{max} - QP_{base}) \cdot R_{i,j}
    \label{eq:qp}
\end{equation}

\begin{itemize}
    \item[QP Mapping] 
    Using codecs that support Quantization Parameter (QP) maps (e.g., x265), we adjust per-block QP offsets proportionally 
    to their removability, as shown in Eq.~\ref{eq:qp}.
    $QP_{base}$ is the baseline QP selected by the codec, $QP_{max}$ is the maximum allowable QP, 
    and $R_{i,j}$ is the removability score for block $(i,j)$, normalized between $[-1, 1]$.
    Given that higher QP values correspond to more aggressive compression and thus greater loss of detail, 
    this approach ensures that less important blocks are compressed more heavily, 
    while critical foreground regions are preserved with higher fidelity.
    QP maps are supposedly supported in AV1 and VVC, but current implementations in FFMPEG do not allow external QP maps; 
    thus, we focus on x265 for this feature.
    Furthermore, even in x265, QP maps are only supported at segment-level granularity, not at the frame-level.
    We could encode each frame with its own QP map, by setting a GOP size of 1, 
    but this would require re-encoding each frame separately, losing inter-frame compression efficiency.
    Thus, we segment videos into GOPs of fixed length (e.g., 8 frames), 
    calculate the average QP for each block across the GOP, and apply the averaged QP map to all frames in a GOP.
    QP maps are saved as grayscale images, with pixel values representing QP offsets.
    \item[Downsampling] 
    Removability scores are converted to integers, from 0 to a maximum value set by the user.
    These are used as downscaling factors, which correspond to exponents of powers of 2 (e.g., 2x, 4x, 8x). 
    This reduces texture detail while maintaining spatial structure.
    PRESLEY then downsamples each block according to its assigned factor using pixel area relation, 
    which gives moire'-free results.
    Downscaling factors are stored in downscale maps as grayscale images, 
    with pixel values indicating the downscaling factor for each block.
    \item[Noise Injection] 
    An alternative degradation uses rounds of Gaussian noise added to each block, 
    where the number of rounds is determined by the block's removability score. 
    This simulates compression artifacts or sensor noise.
    The standard deviation of the Gaussian noise is a user-defined parameter.
    Whether this type of degradation interferes with codec compression efficiency needs further investigation, 
    but initial tests show promising results.
    Noise levels are stored in noise maps as grayscale images, 
    with pixel values indicating the number of rounds of noise applied to each block.
\end{itemize}

\subsubsection{PRESLEY: Encoding}

PRESLEY currently supports only x265 because of QP mapping requirements.
Having already explored H.264 and HNeRV capabilities and challenges in ELVIS,
we consider the results of x265 as sufficiently representative of traditional codecs for PRESLEY's purpose, 
and plan to extend support to AV1, VVC, and neural codecs in future work.

\subsubsection{Metadata Encoding}

ELVIS shrink masks, and PRESLEY greyscale images, also interpretable as masks, \ie, restoration metadata, 
allow the client to know the coordinates of the block removed, 
or how many rounds of degradation each block went through, information necessary and sufficient to restore them.
However, the size of these masks needs to be taken into account. 
The first possibility is to send such masks as a second encoded video, but a lossless encoding would be quite cumbersome, 
and a lossy encoding would introduce errors in the matrix, which would drastically impact in-painting performance.
Therefore, we instead convert the 3D tensor into a 2D matrix, 
where each row is the concatenation of all rows from a single mask, and save this in a single CSV file. 
We lose, therefore, the information about the number of rows in the mask, 
but that can easily be calculated back by the client once the video is decoded, 
by extracting the video resolution and comparing it with the number of values in a row of the 2D Matrix.
Such a CSV file, however, has a size in the same order of magnitude as that of the whole compressed video, 
a testament to the efficiency reached by modern video encoders. 
Thus, the masks CSV file needs to be compressed as well, 
by converting it to a numpy array and saving it in .npz~\cite{numpy} format. 
Aside from reducing the size of this file to at most 5 percent of the encoded video (less if the resolution is high), 
such compression is also very robust and fast for the client to unpack and use. 
A more precise study in metadata compression might reduce their impact even further, 
but is deemed out of scope for the present study.

\subsection{Client-Side Restoration Pipeline}

The first step on the client side, is receiving the degraded video encodings.
On top of that, the client also receives the metadata, \ie the NPZ file with the compressed masks, 
so that it knows which blocks to inpaint, or how many rounds of restoration to apply.
In the case of ELVIS, among the metadata is also the list of parameters that the encoder used to encode the video, 
particularly in the case of learned representations, since the decoder will need to be configured with the same parameters, 
for a successful decoding.
PRESLEY does not require any encoder parameters to be sent, as HNeRV was not implemented for it.

\subsubsection{ELVIS: Decoding}

If the video sent to the client was encoded via a traditional codec such as H.264, 
FFmpeg can directly parse it and divide it into frames to begin restoration. 
If, on the other hand, the video was encoded via a neural codec, such as HNeRV, the client runs the HNeRV decoding model, 
which returns the frames directly and then uses FFmpeg to remove the padding introduced before the encoding. 
Thus, the frames that are made available after the decoding step are equivalent in terms of resolution, 
regardless of whether they came from HNeRV and were therefore padded to avoid HNeRV failures, 
or from a traditional codec where they did not undergo any padding. 
This way from here on, ELVIS is agnostic to which codec was used. 

\subsubsection{ELVIS: Frame Stretching}

\begin{algorithm}[t]
\caption{Frame Stretching}
\label{alg:stretch_frames}
\begin{algorithmic}[1]
\Require $c$: Color channels (3)
\Require $F' \in \mathbb{R}^{I \times J-r \cdot J}$: Shrunk frame
\Require $n$: Frame number
\Require $b$: Block size
\Require $O \in \mathbb{R}^{I/r \times J/r \times N}$: Binary mask tensor
\Ensure $F \in \mathbb{R}^{I \times J \times c}$: Stretched frame
\For{$i = 0$ \textbf{to} $I/b$}
    \For{$j = 0$ \textbf{to} $J/b$}
        \If{$O_{i, j, n}=0$}
            \State $F_{i, j} \gets F'_{i, j}$ \Comment{Fill stretched frame with shrunk block}
        \Else
            \State $F_{i, j} \gets 0$ \Comment{Fill stretched frame with black block}
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/New Figures/stretching&inpainting.pdf}
    \caption{Frame stretching and in-painting.}
    \label{fig:stretching}
\end{figure}

Regardless of where they came from, the decoded shrunk frames need to be stretched, 
\ie brought back to their original resolution, by filling the missing blocks with some temporary pixels 
that the in-painter algorithm will replace, as shown in Figure \ref{fig:blockdropping}(b).
ELVIS decompresses the mask matrix back into a numpy array, converting it back into PNG masks, 
and then parallelizes the stretching by spinning up a processing thread for each pair of mask and shrunk frame.
Each process runs Algorithm~\ref{alg:stretch_frames}, 
therefore process $n$ takes the corresponding shrunk frame $F'_{i, j}$, parses the mask $O_{i, j, n}$ block by block, 
and if the block is black (and therefore not to be in-painted, but instead taken from the shrunk frame) 
it moves the corresponding element of the shrunk frame's into the new stretched frame $F_{i, j}$.
Otherwise, if the block is white, and thus to be inpainted, 
it adds a placeholder black block in the corresponding position of the stretched frame.
Next, the newly generated stretched frame gets saved as PNG in a stretched folder, 
and encoded losslessly via FFmpeg in a \textit{stretched.mp4} video.

\subsubsection{ELVIS: In-painting}

The stretched frames are now ready to be in-painted. 
As previously mentioned, we implemented two generative AI in-painting models, namely ProPainter and E2FGVI, but once again, 
ELVIS is agnostic to the model chosen and can be easily configured to accommodate any in-painter 
that takes as input video frames and masks (with portions of any shape, not only square blocks) and returns in-painted frames.
Regardless of the chosen in-painter model, the orchestrator calls it with the necessary arguments, 
and saves the results in the in-painted folder, then encodes them, once again losslessly, 
into an \textit{in-painted.mp4} video file, via FFmpeg.

\subsubsection{PRESLEY: Decoding}

PRESLEY's client-side decoding is straightforward, as it only involves using FFmpeg to decode the received video into frames.

\subsubsection{PRESLEY: Adaptive Enhancement}

This section contains pseudocode (LaTeX algorithms) and diagrams for the adaptive Real-ESRGAN upscaling algorithm 
and the parallel wrapper that orchestrates per-device upsamplers and chunked processing. 
The TikZ figure illustrates the staged pipeline with overlap+feather to mitigate seam artifacts.
\begin{itemize}
  \item The algorithm pseudocode corresponds to the implementation in the referenced Python file: staged 2$\times$ upscales, block-level replacement using resized original, and device-cached upsamplers in the parallel wrapper.
  \item The overlap+feather variant shows a recommended mitigation for visible seams: when replacing blocks tile-by-tile, extract a small border (overlap) and apply a tapered weight (feather) to smoothly blend with neighboring tiles before cropping to the tile interior.
\end{itemize}

\begin{algorithm}[H]
\caption{Parallel adaptive Real-ESRGAN restoration (wrapper)}
\label{alg:realesrgan-wrapper}
\begin{algorithmic}[1]
\Require
  Input frames directory $\mathcal{D}_{\text{in}}$ containing $N$ frames,\\
  Output directory $\mathcal{D}_{\text{out}}$,\\
  Per-frame downscale maps $\mathbf{M} \in \mathbb{Z}^{N \times n_y \times n_x}$,\\
  Block size $b$, model/configuration parameters,\\
  Device list $\mathcal{G}$, per-device workers $w$, parallel chunk length $L$ (optional)
\Ensure
  Restored frames written to $\mathcal{D}_{\text{out}}$
\State Validate input directory and collect sorted frame paths $P[0\!:\!N-1]$
\State Validate $\mathbf{M}$ has shape $(N,n_y,n_x)$
\State Resolve devices $\mathcal{G}_{\text{resolved}} \leftarrow \text{ResolveDeviceList}(\mathcal{G})$
\State Build device slots $S \leftarrow \{(g,s): g\in\mathcal{G}_{\text{resolved}}, s\in[0..w-1]\}$
\If{$S$ empty} \textbf{raise} "No devices available"
\EndIf
\If{$L$ is None or $L\le 0$} $L \leftarrow \lceil N / |S| \rceil$ \EndIf
\State $L \leftarrow \max(1, \min(L, N))$
\State Partition frames into chunks of length $L$: build chunk list $C$
\State Initialize thread-safe upsampler cache $\mathcal{U}_{\text{cache}} \leftarrow \{\}$ and lock $\ell$
\Function{GetUpsampler}{$g,s$}
  \State $key \leftarrow \text{DeviceSlotKey}(g,s)$
  \State acquire $\ell$
  \If{$key \notin \mathcal{U}_{\text{cache}}$}
    \State instantiate upsampler $up \leftarrow \text{InstantiateRealESRGAN}(g,\dots)$
    \State $\mathcal{U}_{\text{cache}}[key] \leftarrow up$
  \EndIf
  \State release $\ell$
  \State \Return $\mathcal{U}_{\text{cache}}[key]$
\EndFunction
\Function{ProcessChunk}{$chunk=(start,end), g, s$}
  \State $up \leftarrow \text{GetUpsampler}(g,s)$
  \For{$i \leftarrow start$ \textbf{to} end-1}
    \State $p \leftarrow P[i]$; $I \leftarrow \text{ReadImage}(p)$
    \State $M_i \leftarrow \mathbf{M}[i]$
    \State Define $\mathcal{U}_{\text{once}}(X) \triangleq \text{UpsampleWithRealESRGAN}(up,X)$
    \State $\widehat{I} \leftarrow \text{AdaptiveUpscale}(I, M_i, b, \mathcal{U}_{\text{once}})$
    \State write $\widehat{I}$ to corresponding output path
  \EndFor
\EndFunction
\State Launch ThreadPoolExecutor with $T=\min(|S|,|C|)$ workers
\State Round-robin assign chunks to device slots $(g,s)\in S$, submit ProcessChunk tasks
\State Wait for all tasks to complete and propagate exceptions
\State Shutdown executor; return
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Adaptive Real-ESRGAN Upscaling}
\label{alg:adaptive-realesrgan}
\begin{algorithmic}[1]
\Require
  Downsampled image $I_{\text{low}}\in\mathbb{R}^{H\times W\times 3}$,\\
  Downscale map $D\in\mathbb{Z}_{\ge0}^{n_y\times n_x}$ (log$_2$ downscale per block),\\
  Block side length $b\in\mathbb{Z}_{>0}$ (so $n_y = H/b$, $n_x = W/b$),\\
  Upsample callable $\mathcal{U}:\mathbb{R}^{h\times w\times 3}\to\mathbb{R}^{2h\times 2w\times 3}$ (default: Real-ESRGAN 2x)
\Ensure
  Restored image $\widehat{I}\in\mathbb{R}^{H\times W\times 3}$
\State // Convert log2 map to factor map
\State $F \leftarrow 2^{D}$ \Comment{elementwise; $F_{i,j}\in\{1,2,4,\dots\}$}
\State $m \leftarrow \max_{i,j} F_{i,j}$
\If{$m = 1$}
  \State \Return $I_{\text{low}}$ \Comment{no adaptive upscaling required}
\EndIf
\State // Start from the coarsest resolution
\State $I^{(0)} \leftarrow \operatorname{Resize}\big(I_{\text{low}},(W/m,\;H/m)\big)$ \Comment{area downsample}
\State $t \leftarrow \log_2 m$  \Comment{number of 2x stages}
\For{$k \leftarrow 1$ \textbf{to} $t$} \Comment{coarse $\to$ fine}
  \State $\tilde I \leftarrow \mathcal{U}\big(I^{(k-1)}\big)$ \Comment{global 2$\times$ upscaling}
  \State $\phi \leftarrow 2^{t-k}$ \Comment{restore threshold}
  \State $b_{\text{cur}} \leftarrow b / \phi$
  \State $\mathcal{B} \leftarrow \mathrm{SplitBlocks}(\tilde I, b_{\text{cur}})$
  \State $I_{\mathrm{resized}} \leftarrow \operatorname{Resize}\big(I_{\text{low}},\; \mathrm{size}(\tilde I)\big)$
  \State $\mathcal{B}_{\text{orig}} \leftarrow \mathrm{SplitBlocks}(I_{\mathrm{resized}}, b_{\text{cur}})$
  \For{each block index $(i,j)$}
    \If{$F_{i,j} \le \phi$}
      \State $\mathcal{B}[i,j] \leftarrow \mathcal{B}_{\text{orig}}[i,j]$
    \Else
      \State $F_{i,j} \leftarrow \phi$
    \EndIf
  \EndFor
  \State $I^{(k)} \leftarrow \mathrm{CombineBlocks}(\mathcal{B})$
\EndFor
\State \Return $I^{(t)}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Adaptive Deblurring Restoration}
\label{alg:adaptive-deblurring}
\begin{algorithmic}[1]
\Require Input frames directory $\mathcal{D}_{\text{in}}$, output directory $\mathcal{D}_{\text{out}}$, noise maps $\mathbf{N}\!\in\!\mathbb{Z}^{N\times n_y\times n_x}$ (rounds per block), block size $b$, ordered noise levels $0 = \eta_0 < \eta_1 < \dots < \eta_L$, deblurring model $\mathcal{B}(\cdot,\eta)$
\Ensure Restored frames in $\mathcal{D}_{\text{out}}$
\State Collect sorted frame paths $P[0{:}N-1]$ and verify $\mathbf{N}$ shape
\State $\kappa \leftarrow \max(\mathbf{N})$ \Comment{number of degradation rounds}
\For{$\ell \leftarrow 1$ \textbf{to} $\kappa$}
  \State $\Delta_\ell \leftarrow \{(i,j,n) \mid \mathbf{N}[n,i,j] \ge \ell\}$
  \If{$\Delta_\ell = \emptyset$} \textbf{continue}
  \EndIf
  \For{$n \leftarrow 0$ \textbf{to} $N-1$}
    \State $I \leftarrow \text{ReadImage}(P[n])$
    \State $\widehat{I} \leftarrow \mathcal{B}\big(I,\,\eta_\ell\big)$ \Comment{global pass at noise level $\eta_\ell$}
    \State $\mathcal{B}_{\text{tiles}} \leftarrow \text{SplitBlocks}(\widehat{I}, b)$
    \State $\mathcal{O}_{\text{tiles}} \leftarrow \text{SplitBlocks}(I, b)$
    \For{each block index $(i,j)$}
      \If{$(i,j,n) \in \Delta_\ell$}
        \State $\mathcal{O}_{\text{tiles}}[i,j] \leftarrow \text{FeatherBlend}\big(\mathcal{O}_{\text{tiles}}[i,j],\; \mathcal{B}_{\text{tiles}}[i,j]\big)$
        \State $\mathbf{N}[n,i,j] \leftarrow \mathbf{N}[n,i,j] - 1$
      \EndIf
    \EndFor
    \State $I_{\text{upd}} \leftarrow \text{CombineBlocks}(\mathcal{O}_{\text{tiles}})$
    \State $\text{WriteImage}(P[n], I_{\text{upd}})$
  \EndFor
\EndFor
\State Copy restored frames to $\mathcal{D}_{\text{out}}$; return
\end{algorithmic}
\end{algorithm}

Based on the degradation strategy used at the server, PRESLEY applies the corresponding restoration method:
\begin{itemize}
    \item[Super-Resolution] 
    If blocks were downsampled, PRESLEY restores them through progressive super-resolution rounds.
    The client first decompresses the downscale maps from the NPZ file, 
    then downsamples the entire frame to the maximum downsampling factor used.
    It then applies a super-resolution model (e.g., Real-ESRGAN~\cite{realesrgan}) to upsample the full frame by 2x,
    the details of which are explained in Algorithm~\ref{alg:realesrgan-wrapper}.
    After each round, blocks that were downsampled to that scale or higher are extracted and finalized.
    The full process is delineated in Algorithm~\ref{alg:adaptive-realesrgan} 
    and repeats until all blocks are restored to their intended resolution.
    \item[Deblurring] 
    If blocks were degraded via Gaussian noise injection, PRESLEY performs iterative generative deblurring 
    as summarized in Algorithm~\ref{alg:adaptive-deblurring}. 
    The client first decompresses the noise maps from the NPZ file and uses them to schedule adaptive restoration rounds.
    For each round $\ell$ the client performs a global pass of the trained deblurring model (e.g., InstantIR~\cite{instantir}) 
    at the corresponding noise level $\eta_\ell$, splits both the model output and the original frame into $b\times b$ tiles, 
    and finalizes tiles whose remaining noise rounds match the current threshold by feather-blending the restored tile into the output frame.
    Finalized tiles are written back and removed from further processing; 
    the procedure repeats for progressively less degraded tiles until all blocks are restored.
\end{itemize}

\subsection{Quality measurement}
The last component of the orchestrator script is the quality measurement module. 
This is the reason for having previously processed the reference and benchmark videos. 
We compare the reference video with the benchmark (whose only difference is that the former is encoded losslessly, 
while the latter is encoded at the bitrate calculated previously), and again the reference with the in-painted 
(whose frames were shrunk, encoded at the same bitrate as the benchmark, decoded, stretched, and in-painted) 
to gauge which process resulted in a better quality video at a given bitrate. 
Consequently, our goal is to have an in-painted video that presents a better trade-off of computational cost and bandwidth requirements, 
compared to the benchmark video.
We use the ffmpeg-quality-metrics~\cite{ffmpeg_quality_metrics} Python package to compute MSE~\cite{mse}, PSNR~\cite{psnr}, 
SSIM~\cite{ssim}, and VMAF~\cite{vmaf}, and add to those the more recent and generated image-based LPIPS metric~\cite{lpips},
and the FVMD metric~\cite{fvmd}, which is specifically designed for video quality assessment of AI-generated videos.

After computing these metrics, they are saved in a single \textit{in-painted-metrics.csv} file, 
and finally gathered by the Python script \textit{collect-metrics}, together with the time required for each step of the processing, 
and the list of arguments that identify the experiment, in a \textit{experiment-results.csv} file.
This file, together with the benchmark and inpainted videos, is considered the final output of this implementation of ELVIS, 
and can be queried (for example, via a Python interactive notebook), for insights into the results of each experiment.
For example, one might be interested in which combination of parameters shows more promise and how well the in-painting process stacks 
against the SOTA encoders.

\section{Performance Evaluation} \label{sec:evaluation}

An integral part of the architecture is monitoring the behavior and results of each component, to identify bottlenecks and improvements. 
Monitoring spans both server and client components, recording the time taken by each pipeline stage and evaluating the quality of the final output.

To assess the effectiveness of the pipeline, we run experiments in which the in-painted video is compared against a benchmark video 
encoded at the same resolution and file size but without client-side enhancements, 
across a range of metrics traditionally used for video evaluation (MSE, PSNR, SSIM, VMAF) 
as well as metrics specifically suited for in-painted content (LPIPS, FVMD). 
This allows us to quantify the benefits of the restoration process 
and determine whether the experiment's parameter configuration has resulted in meaningful improvements.

We conducted a series of experiments to evaluate the performance, by running the pipeline on a Ubuntu 20.04 LTS server, 
equipped with an Intel Xeon Gold 5218 CPU with 64 cores running at \qty{2.30}{GHz}, and two NVIDIA Quadro RTX 8000 GPUs having \qty{48}{GB} of memory. 
The experiments were designed to test each major component: degradation, encoding, metadata compression, restoration, and quality measurement, 
across a varied range of parameter configurations, shown in Table~\ref{tab:parameters}. 
We processed sequences from the DAVIS dataset~\cite{davis}, known for its scene variety in terms of compositions and camera motions, 
and resolutions in line with video streaming requirements (480p and 1080p).
Given the computationally intensive nature of this task, we ended up running \num{200} valid experiments on \num{10} different videos, 
which we consider a varied dataset from which to draw valuable insights.

The method presented so far, by which we runs several tests on each video sequence, each with a different configuration of parameters, 
is akin to training phase of a machine learning model. 
Indeed, in both cases the model is looking for the optimal configuration of its parameters that result in the best output value, 
in this case, the best improvement over benchmark video quality. 
Therefore, the more time is available on the server side to compute experiments on a particular video sequence, 
the higher the likelihood that it will find a configuration that improves the quality significantly. 

\subsection{ELVIS Results}

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{Figures/Correlation Matrix.pdf}
    \caption{Impact of each parameter on quality metrics}
    \label{fig:correlation}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{Figures/VMAF-by-block-size.png}
%     \caption{VMAF comparison at different block sizes}
%     \label{fig:VMAF-by-block-size}
% \end{figure}

Figure~\ref{fig:correlation} presents the Pearson correlation between ELVIS’s parameters and key quality metrics, 
including encoding and in-painting times. 
This analysis provides insights into how parameter configurations impact final video quality. 
As expected, bitrate and the fraction of blocks removed exhibit the strongest correlations. 
Higher bitrates improve video quality but increase computational demands, while larger removed fractions degrade quality 
by making precise in-painting more difficult. 
Interestingly, the correlation between the removed fraction and modern metrics like VMAF and LPIPS is limited, 
but a thorough investigation is deemed beyond the scope of this work.

\begin{figure}[b]
    \centering\includegraphics[width=\columnwidth]{Figures/VMAF-Alpha.pdf}
    \vspace{-4mm}
    \caption{VMAF distribution over $\alpha$.}
    \label{fig:vmaf-alpha}
    \vspace{-4mm}
\end{figure}

Figure~\ref{fig:vmaf-alpha} shows how variations in $\alpha$, the spatial-to-temporal complexity weighting parameter, affect VMAF scores. 
An interesting trend is observed: values of $\alpha$ toward the extremes, which emphasize either spatial or temporal complexity in block removal decisions, tend to result in better video quality. 
Similar trends were observed for $\beta$, which controls temporal smoothing and block size.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/comparison.pdf}
    \caption{Comparison of benchmark and in-painted metrics.}
    \label{fig:comparison}
\end{figure*}

Figure~\ref{fig:comparison} compares quality metrics for benchmark and in-painted videos across all experiments. 
Note that, for MSE and LPIPS, lower means better, while for PSNR, SSIM, and VMAF, higher means better.
For most cases, in-painting improves VMAF and LPIPS scores, which are the metrics most attuned to human perception, with gains of up to 10 VMAF points and 0.2 LPIPS points. 
Additionally, the overall quality tends to be relatively low, primarily due to the significant difference in resolution between the uncompressed input video and the benchmark and inpainted videos.
However, some experiments showed reduced quality, in which cases, ELVIS would transmit the benchmark video, ensuring that in-painting never degrades video quality for viewers.

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{Figures/New Figures/VMAF_difference.png}
    \caption{VMAF improvement of in-painted over benchmark, by video name.}
    \label{fig:VMAF-by-title}
    \vspace{-5mm}
\end{figure}

Figure~\ref{fig:VMAF-by-title} highlights VMAF improvements by video title. On average, in-painting yields an increase of 2-3 VMAF points, with some videos benefiting from improvements of up to \num{11} points, corresponding to a very noticeable improvement.

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{Figures/New Figures/time_complexity.png}
    \caption{Time complexity of encoding and in-painting, by video name.}
    \label{fig:time-complexity}
    \vspace{-5mm}
\end{figure}

Figure~\ref{fig:time-complexity} depicts the time complexity of ELVIS’s encoding and in-painting stages. 
Generative AI models, while powerful, are not yet optimized for efficiency and require significant GPU power, making higher resolution videos impossible to process.
Experiments often took \num{10} times longer than the video’s duration to complete in-painting, rendering real-time application infeasible on current consumer systems. 
Furthermore, encoding with HNeRV was an order of magnitude slower than in-painting, likely as a combination of the complexity of HNeRV and sub-optimal configuration inside of ELVIS for lack of documentation. Unfortunately, we could not reach the authors for help.
This highlights the need for advancements in both in-painting and neural encoding to support real-time streaming.

\subsection{PRESLEY Results}

