\section{ELVIS Implementation}
\label{sec:implementation}

\subsection{AI video encoding pipeline -- client side}
The first step on the client side, is receiving the shrunk and benchmark video encodings.
On top of that, the client also receives the metadata, \ie the NPZ file with the compressed shrinking masks, so that it knows where to add temporary blocks to the shrunk frames.
Thus, the in-painting stage can occur properly. 
Among the metadata is also the list of parameters that the encoder used to encode the video, particularly in the case of learned representations, since the decoder will need to be configured with the same parameters, for a successful decoding.

\textbf{Decoding}.
If the video sent to the client was encoded via a traditional codec such as AVC, FFmpeg can directly parse it and divide it into frames to reverse shrinking. 
If, on the other hand, the video was encoded via a neural codec, such as HNeRV, the client runs the HNeRV decoding model, which returns the frames directly and then uses FFmpeg to remove the padding introduced before the encoding. 
Thus, the frames that are made available after the decoding step are equivalent in terms of resolution, regardless of whether they came from HNeRV and were therefore padded to avoid HNeRV failures, or from AVC where they did not need any padding. 
This way from here on, ELVIS is agnostic to which codec was used. 

\textbf{Frame Stretching}.
Regardless of where they came from, the decoded shrunk frames need to be stretched, \ie brought back to their original resolution, by filling the missing blocks with some temporary pixels that the in-panter algorithm will replace.
To do this, a Python script \textit{stretch-frames} is called, to reverse the process of \textit{shrink-frame}, as shown in Figure \ref{fig:blockdropping}(b).
Its task is decompressing the mask matrix back into a numpy array, converting it into the PNG masks that we started with, and then parallelizing the stretching by spinning up a processing thread for each pair of mask and shrunk frame.
Each process runs Algorithm~\ref{alg:stretch_frames}, therefore process $n$ takes the corresponding shrunk frame $F'_{i, j}$, parses the mask $O_{i, j, n}$ block by block, and if the block is black (and therefore not to be in-painted, but instead taken from the shrunk frame) it moves the corresponding element of the shrunk frame's into the new stretched frame $F_{i, j}$.
Otherwise, if the block is white, and thus to be inpainted, it adds a placeholder black block in the corresponding position of the stretched frame.
Next, the newly generated stretched frame gets saved as PNG in a stretched folder, and encoded losslessly via FFmpeg in a stretched MP4 video.

\textbf{In-painting}.
The stretched frames are now ready to be in-painted. 
As previously mentioned, we implemented two generative AI in-painting models, but once again, ELVIS is agnostic to the model chosen and can be easily configured to accommodate any in-painter that takes as input video frames and masks (with portions of any shape, not only square blocks) and returns in-painted frames.
Regardless of the chosen in-painter model, the orchestrator calls it with the necessary arguments, and saves the results in the in-painted folder, then encodes them, once again losslessly, into an in-painted MP4 video file, via FFmpeg.

\subsection{Quality measurement}
The last component of the orchestrator script is the quality measurement module. 
This is the reason for having previously processed the reference and benchmark videos. 
We compare the reference video with the benchmark (whose only difference is that the former is encoded losslessly, while the latter is encoded at the bitrate calculated previously), and again the reference with the in-painted (whose frames were shrunk, encoded at the same bitrate as the benchmark, decoded, stretched, and in-painted) to gauge which process resulted in a better quality video at a given bitrate. 
Consequently, our goal is to have an in-painted video that presents a better trade-off of computational cost and bandwidth requirements, compared to the benchmark video.
We use the ffmpeg-quality-metrics~\cite{ffmpeg_quality_metrics} Python package to compute MSE~\cite{mse}, PSNR~\cite{psnr}, SSIM~\cite{ssim}, and VMAF~\cite{vmaf}, and add to those the more recent and generated image-based LPIPS metric~\cite{lpips}.

After computing these metrics, they are saved in a single CSV file \textit{in-painted-metrics} file, and finally gathered by the Python script \textit{collect-metrics}, together with the time required for each step of the processing, and the list of arguments that identify the experiment, in a single CSV file \textit{experiment-results} file.
This file, together with the benchmark and inpainted videos, is considered the final output of this implementation of ELVIS, and can be queried (for example, via a Python interactive notebook), for insights into the results of each experiment.
For example, one might be interested in which combination of parameters shows more promise and how well the in-painting process stacks against the SOTA encoders.

\section{Performance Evaluation}
\label{sec:evaluation}

\begin{table}[t]
\caption{ELVIS configuration parameters and possible instantiations.}
\vspace{-4mm}
\begin{tabular}{ll}
\hline
Parameter        & Values          \\ \hline
Video            & DAVIS dataset   \\
Block Size       & 8, 16, 32, 64          \\
Removed Fraction & 0.25, 0.50, 0.75       \\
$\alpha$         & 0, 0.25, 0.50, 0.75, 1 \\
$\beta$          & 0, 0.25, 0.50, 0.75, 1 \\
Height           & 360, 540, 720, 900     \\
Width            & 640, 960, 1280, 1600   \\
Codec            & AVC, HNeRV   \\
In-painter Tool  & ProPainter, E2FGVI
\end{tabular}
\vspace{-4mm}
\label{tab:parameters}
\end{table}

An integral part of the ELVIS architecture is monitoring the behavior and results of each component, to identify bottlenecks and improvements. Monitoring spans both server and client components, recording the time taken by each pipeline stage and evaluating the quality of the final output.

\textbf{Benchmark Comparisons.}
To assess the effectiveness of the pipeline, we run experiments in which the in-painted video is compared against a benchmark video encoded at the same resolution and file size but without client-side enhancements, across a range of metrics traditionally used for video evaluation (MSE, PSNR, SSIM, VMAF) as well as metrics specifically suited for in-painted content (LPIPS). 
This allows ELVIS to quantify the benefits of its in-painting process and determine whether the experiment's parameter configuration has resulted in meaningful improvements.

\textbf{Experiment Setup.}
We conducted a series of experiments to evaluate the performance of ELVIS, by running the pipeline on a Ubuntu 20.04 LTS server, equipped with an Intel Xeon Gold 5218 CPU with 64 cores running at \qty{2.30}{GHz}, and an NVIDIA Quadro RTX 8000 GPU having \qty{48}{GB} of memory. 
The experiments were designed to test each major component: encoding, in-painting, and quality measurement, across a varied range of parameter configurations, shown in Table~\ref{tab:parameters}. 
We processed sequences from the DAVIS dataset~\cite{davis}, known for its scene variety in terms of compositions and camera motions, and resolutions in line with video streaming requirements (480p and 1080p).
Given the computationally intensive nature of this task, we ended up running \num{200} valid experiments on \num{10} different videos, which we consider a varied dataset from which to draw valuable insights.

\section{Discussion}
% discuss how the phase we explain is akin to the training phase of a model. We are running tests to find the right combination of parameters that offers the best decoding quality, and that can then be used in production. This way, even though ELVIS is often worse than HNeRV, we only care about when it is better, as that is the configuration of parameters that the user would send to the client.
The method presented so far, by which ELVIS runs several tests on each video sequence, each with a different configuration of parameters, is akin to training phase of a machine learning model. Indeed, in both cases the model is looking for the optimal configuration of its parameters that result in the best output value, in this case, the best improvement over benchmark video quality. Therefore, the more time is available on the server side to compute experiments on a particular video sequence, the higher the likelihood that ELVIS will find a configuration that improves the quality significantly. 

%The size of metadata
%Customize the in-painting network
%ELVIS and other Encoding algorithms
%EMRACE compare with super-resolution techniques
% we need to mention how ELVIS would be used: the server would run many experiments on the same segment of a video, find the best combination of parameters at each bitrate and make a manifest from them. We can also mention how this choice could be improved by knowing the computation capabilities of the client, but that we assume the clients have enough GPU power and we leave the exploration of this question for future efforts.

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{Figures/Correlation Matrix.pdf}
    \caption{Impact of each parameter of ELVIS on quality metrics}
    \label{fig:correlation}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figures/VMAF-by-block-size.png}
    \caption{VMAF comparison at different block sizes}
    \label{fig:VMAF-by-block-size}
\end{figure}


\subsection{Quality Metrics}
To assess the visual quality and accuracy of our video reconstruction pipeline, we employed a range of metrics traditionally used for video evaluation (MSE, PSNR, SSIM, VMAF) as well as metrics specifically suited for in-painted content (LPIPS).
\begin{enumerate}[leftmargin=*]
    \item MSE (Mean Squared Error)~\cite{mse}: MSE calculates the average squared difference between original and reconstructed frames. This metric helps identify areas of degradation, providing a direct measure of pixel-level errors.
    \item PSNR (Peak Signal-to-Noise Ratio)~\cite{psnr}: This metric quantifies the pixel-level fidelity of the reconstructed video compared to the original, measuring the intensity differences between corresponding pixels in terms of decibels (dB). Higher PSNR values indicate closer similarity to the original.
    \item SSIM (Structural Similarity Index)~\cite{ssim}: SSIM assesses the structural preservation of the reconstructed frames by comparing luminance, contrast, and structural information. SSIM values range from 0 to 1, with values closer to 1 indicating higher similarity to the original in terms of structural content.
    \item VMAF (Video Multi-Method Assessment Fusion)~\cite{vmaf}: Developed by Netflix, VMAF is a perceptual video quality metric that combines human vision modeling with multiple quality assessment algorithms. It is particularly effective for predicting human-perceived quality and how in-painting might affect the overall perceptual experience of the video.
    \item LPIPS (Learned Perceptual Image Patch Similarity)~\cite{lpips}: LPIPS is specifically suited for in-painted content, as it calculates feature-level similarity by comparing activations from deep neural networks. This metric is highly sensitive to perceptual and structural discrepancies, making it valuable for assessing the fidelity of in-painted regions.
\end{enumerate}

\subsection{Results}

% Figure~\ref{fig:correlation} shows the Pearson correlation among each parameter of ELVIS, and each quality metric, including encoding and in-painting times. 
% This allows us to gauge the impact of different parameter configurations on the final video quality. 
% As shown, every parameter has a moderate impact on at least one quality metric, but bitrate and the fraction of blocks removed have, as expected, a disproportionate impact, with higher bitrate increasing the final quality of the video, but also the computation demands, and the removed fraction having the opposite effect, by making the video harder to in-paint precisely at the client side, and therefore resulting in lower quality. 
% However, it is interesting how limited the correlation is between the removed fraction of blocks and the more modern metrics, \ie VMAF and LPIPS. 
% However, an investigation into the reasons for such counter-intuitive behavior is deemed outside the scope of the present work, and left for future efforts. 

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{Figures/Correlation Matrix.pdf}
    \caption{Impact of each parameter of ELVIS on quality metrics.}
    \label{fig:correlation}
    \vspace{-4mm}
\end{figure}

\textbf{Correlation Analysis.} Figure~\ref{fig:correlation} presents the Pearson correlation between ELVIS’s parameters and key quality metrics, including encoding and in-painting times. 
This analysis provides insights into how parameter configurations impact final video quality. 
As expected, bitrate and the fraction of blocks removed exhibit the strongest correlations. 
Higher bitrates improve video quality but increase computational demands, while larger removed fractions degrade quality by making precise in-painting more difficult. 
Interestingly, the correlation between the removed fraction and modern metrics like VMAF and LPIPS is limited, but a thorough investigation is deemed beyond the scope of this work.

% With regards to the other parameters, Figure~\ref{fig:vmaf-alpha} shows the impact that setting different values of $\alpha$  has on VMAF. 
% The figure delineates a tentative upward trend, already quantified by the linear correlation matrix, \ie higher values of $\alpha$  (which correspond to higher contribution in block removal decision, given to spatial complexity than to temporal complexity) tend to result in higher VMAF.
% The other parameters, block size and $\beta$ , follow a similar trend (positive linear relationships of higher values of $\beta$  and lower values of block size, with higher quality.

\begin{figure}[b]
    \centering\includegraphics[width=\columnwidth]{Figures/VMAF-Alpha.pdf}
    \vspace{-4mm}
    \caption{VMAF distribution over $\alpha$.}
    \label{fig:vmaf-alpha}
    \vspace{-4mm}
\end{figure}

\textbf{Parameter Impact.} Figure~\ref{fig:vmaf-alpha} shows how variations in $\alpha$, the spatial-to-temporal complexity weighting parameter, affect VMAF scores. 
An interesting trend is observed: values of $\alpha$ toward the extremes, which emphasize either spatial or temporal complexity in block removal decisions, tend to result in better video quality. 
Similar trends were observed for $\beta$, which controls temporal smoothing and block size.

% Figure~\ref{fig:comparison} shows the comparison across quality metrics, between the benchmark video and the in-painted video for each experiment. 
% The dots in the upper portion of the graph represent experiments where the in-painted score is higher than the benchmark, and vice versa. 
% Note that, for MSE and LPIPS, lower means better, while for PSNR, SSIM, and VMAF, higher means better.
% We can see how for most experiments, the in-painting VMAF and LPIPS are better, while other metrics give more mixed results. 
% Considering VMAF and LPIPS are the SOTA metrics for video quality and generated images, respectively, we consider them better suited to gauge ELVIS's ability to improve upon benchmark videos.
% The distance between the dots and the red line is then representative of the improvement brought by ELVIS on the non-enhanced video, which can be seen to amount to up to 10 VMAF points, or 0.2 LPIPS points. 
% Still, we also note how a portion of video qualities have actually been reduced as a consequence of in-painting. 
% However, in case a video would have a lower quality after in-painting, the benchmark video can be sent to client instead, therefore implementing ELVIS never results in a lower quality than otherwise. 
% We still consider the experiments with negative results as relevant to get a complete picture of the capabilities of the current encoding and in-painting landscape.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/comparison.pdf}
    \caption{Comparison of benchmark and in-painted metrics.}
    \label{fig:comparison}
\end{figure*}

\textbf{Benchmark Comparison.}
Figure~\ref{fig:comparison} compares quality metrics for benchmark and in-painted videos across all experiments. 
Note that, for MSE and LPIPS, lower means better, while for PSNR, SSIM, and VMAF, higher means better.
For most cases, in-painting improves VMAF and LPIPS scores, which are the metrics most attuned to human perception, with gains of up to 10 VMAF points and 0.2 LPIPS points. 
Additionally, the overall quality tends to be relatively low, primarily due to the significant difference in resolution between the uncompressed input video and the benchmark and inpainted videos.
However, some experiments showed reduced quality, in which cases, ELVIS would transmit the benchmark video, ensuring that in-painting never degrades video quality for viewers.

% Figure~\ref{fig:VMAF-by-title} also shows the increase in VMAF of in-painted videos over benchmark videos, but splits them by video title, therefore showing directly how much ELVIS improves each video, both on average and maximally. The average improvement is between 1 and 2 VMAF points. 
% However, certain videos can be improved by much more (up to 11 VMAF points), corresponding to a very noticeable improvement. Once again, certain experiments show a reduction in VMAF scores compared to the benchmark and are shown for completeness, but would, in a practical implementation, not be sent to the client, but replaced with their benchmark videos.

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{Figures/New Figures/VMAF_difference.png}
    \caption{VMAF improvement of in-painted over benchmark, by video name.}
    \label{fig:VMAF-by-title}
    \vspace{-5mm}
\end{figure}

\textbf{Video-specific Improvements.}
Figure~\ref{fig:VMAF-by-title} highlights VMAF improvements by video title. On average, in-painting yields an increase of 2-3 VMAF points, with some videos benefiting from improvements of up to \num{11} points, corresponding to a very noticeable improvement.

% %Time complexity of ELVIS
% Figure~\ref{fig:time-complexity} introduces a dimension to the evaluation of the experiments that has not been discussed previously, their time complexity. As a consequence of their initial stage of development, generative AI models are not yet well optimized, especially in terms of computing resources, and therefore, take a significant time to run. 
% % show time complexity, and talk about it, connecting into this:
% The GPU itself, while very capable, was the main bottleneck to the experiments, making higher resolution videos impossible to process for lack of memory. 
% % this can be a moment to reference the table, since we dropped the explanation of resolutions from hnerv encoding
% Besides this, the experiments that could run took about 10 times longer than their video lengths to be in-painted, rendering a real-time application currently impossible, outside the realm of multi-GPU systems, not available to the vast majority of consumers. 
% To allow video in-painting to be used in real-time applications, lots of optimization efforts are required on the video in-painting side.
% We, however, consider that in-panting time pales in comparison to HNeRV encoding and decoding times, which took an order of magnitude more time still. 
% It is hard to tell whether this time is to be input to the complexity of HNeRV itself or a sub-optimal configuration of it inside of ELVIS, as lack of documentation makes the exploration of its parameters complex, and the authors have not responded to our request for clarification.

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{Figures/New Figures/time_complexity.png}
    \caption{Time complexity of encoding and in-painting, by video name.}
    \label{fig:time-complexity}
    \vspace{-5mm}
\end{figure}

\textbf{Time Complexity.}
Figure~\ref{fig:time-complexity} depicts the time complexity of ELVIS’s encoding and in-painting stages. 
Generative AI models, while powerful, are not yet optimized for efficiency and require significant GPU power, making higher resolution videos impossible to process.
% this can be a moment to reference the table, since we dropped the explanation of resolutions from hnerv encoding
Experiments often took \num{10} times longer than the video’s duration to complete in-painting, rendering real-time application infeasible on current consumer systems. 
Furthermore, encoding with HNeRV was an order of magnitude slower than in-painting, likely as a combination of the complexity of HNeRV and sub-optimal configuration inside of ELVIS for lack of documentation. Unfortunately, we could not reach the authors for help.
This highlights the need for advancements in both in-painting and neural encoding to support real-time streaming.

% Having explored the capabilities of ELVIS in depth, we can now outline how such a system would be used in a video streaming pipeline:
% The server would receive video segments to store in multiple representations of bitrate and resolutions, as is already the case nowadays to allow for adaptive streaming. 
% On these representations, it would start running experiments in times where it would otherwise be idle, to find the configuration of parameters that can improve quality via in-painting while maintaining the same bitrate. 
% Once a representation is requested by a client, the server would compare the various experiments that have been run so far, on the representation chosen by the client based on its ABR algorithm, and find the experiment that resulted in the best quality.
% This could be a video to be in-painted by the client, or the original non-in-painted video, in case no experiments were run, or in case no parameter configurations were found that resulted in a better quality than the benchmark video, and send that video to the client. 
% We are here assuming that the requesting client would have the resources to in-paint the segment it requested, which, as we already mentioned, is currently very unlikely.
% However, we consider this optimization process outside the scope of ELVIS, given its ability to incorporate any encoding and in-painting model, as more capable and faster ones become available.

\section{Conclusions and Future Work}
\label{sec:conclusions}

% This paper showed how generative AI models, while yet to be exploited in the context of video streaming, can offer significant improvements to quality without increases in bitrate. 
% Furthermore, we proposed ELVIS, an end-to-end architecture capable of repurposing general in-painting models as they become available and increasingly powerful and efficient, leveraging them for video streaming tasks.

% As the generative AI landscape matures and gets optimized, ELVIS stands as a main component of future video streaming pipelines, capable of running powerful client-side models to significantly improve video streaming user experience, without increasing the amount of information that needs to be exchanged over the Internet.

This paper explored how generative AI models, unexplored in the field of video streaming, can significantly enhance video quality without increasing bitrate. 
We introduced ELVIS, an end-to-end architecture designed to repurpose general in-painting models for streaming tasks, offering a modular framework capable of integrating more powerful and efficient AI technologies as they emerge.
In a practical deployment, ELVIS would exploit server idle times, by identifying parameter configurations that optimize quality through in-painting.
Upon a client request, the server would select the best-performing configuration for delivery.
By leveraging idle server resources and ensuring fallbacks to non-inpainted videos when necessary, ELVIS guarantees that its implementation does not result in quality degradation.

While this work establishes a strong foundation, several future research directions remain to be explored. 
First, optimizing the efficiency of generative in-painting models is critical to reducing computational demands and enabling real-time applications. 
Additionally, understanding the limited correlation between certain configurations and quality metrics could lead to the introduction of different parameters and new frame shrinking strategies. 
Improving metadata compression techniques could further minimize transmission overhead, while adapting ELVIS to better support neural codecs like HNeRV, or introducing new codecs, could unlock new capabilities. 
Finally, a deeper exploration of client-side integration, including the development of lightweight in-painting models for resource-constrained devices, would significantly expand the practical applicability of ELVIS.

As the generative AI landscape evolves, ELVIS stands poised to become a key component of future video streaming systems. 
By combining advanced client-side enhancements with server-side optimizations, it offers the potential to improve user experience without increasing bandwidth consumption, marking a significant step forward in video streaming technology.

\section{Whatever}